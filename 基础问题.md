- 防止过拟合分方法有哪些
  - 从数据角度来看：1）增大数据量；2）使用数据增强技术
  - 从模型结构来说：1）使用简单的模型，可能会欠拟合；2）设计出更优质的网络模型
  - 从网络数据流传输流程来看：
    - 1）卷积操作：
      - 使用空洞卷积
- 如何选取卷积核                          
  -   三个3\*3的卷积核相当于一个7\*7 参数量少了一半
- 1x1卷积核作用                           
  -  1）降维或者升维  
  - 2）减少参数量
- L1、L2正则化作用
  - **L1正则化产生稀疏的权值**
    - 由于其导数为常数，在反向传播中对loss求导
- 梯度消失与梯度爆炸的原因
- 介绍主要的特征提取网络
- 常见的pooling技术有哪些
- 常见的优化器有哪些
- 常见的学习率控制方法有哪些
- 常见的卷积操作有哪些
- 样本不均衡有哪些控制方法
- 常见的损失函数有哪些
- 为什么分类任务不推荐使用MSE
- 手动推到一边反向传播
- 卷积核计算公式？
- 空洞卷积作用
- 深度可分离卷积作用
- batch_size在反向传播中的作用：
  - 一组batch数据前向传播，计算每一个样本对应的loss，反向传播的时候将整个batch求出的loss以样本为单位，在样本的特征维度上求和之后除以batch，然后进行反向传播，从而实现batch的权重共享，[参考资料](https://blog.csdn.net/Boys_Wu/article/details/112752625)
  - **建议使用numpy实现神经网络的设计，以便更好了解其中原理**
- BN操作的作用与原理，BN操作中的超参数介绍
  - **BN操作是无法阻止过拟合，只能拖延过拟合的出现时间**，所以对防止过拟合有一定作用，主要是为了**收束可训练参数**增加系统鲁棒性，然后**加速model收敛**
  - pytorch中的BN操作的父类是nn.Module，说明他是可以训练的
  - BN中有两个重要的超参数：扩张+平移参数
    - 通过BN提供的开关调用开启或者关闭超参数的学习，如果启用 ，反向传播的时候就会更新
    - BN操作有可能把本来分布合理的数据强行归一化到更小的区间，这个时候使用可训练的扩张平移超参数就能有效的还原这种不需要的操作，是的BN合理的享受参数归一化带来的好处
- BN反向传播求导
  - [资料](https://zhuanlan.zhihu.com/p/373360231)
  - 本质还是链式求导：因为![[公式]](https://www.zhihu.com/equation?tex=%5Cgamma)是对真个batch起作用的，所以其偏导应该是batch中所有样本对其偏导之和
- model.eval()作用

  - val与test数据集中调用
  - 1）针对BN操作；2）针对DropOut操作
  - 一般用在前向推理过程中使用，前向推理时使用训练集中生成的均值与方差，也就是保证BN层中的均值与方差不变，如果不启用这个功能，使用test测试的时候BN中的**扩张、平移**参数就会随之改变；
  - 利用全部神经元，不进行神经元的随机丢弃
- maxpooling可以训练吗，他是如何传递导数的

  - [参考资料1](https://blog.csdn.net/Jason_yyz/article/details/80003271)，[参考资料2](https://zhuanlan.zhihu.com/p/258604402)
  - **遵守传播梯度总和保持不变的原则**
  - **pooling没有训练参数**，前向传播的时候记录index索引，反向传播的时候根据索引还原位置信息
  - 无论max pooling还是mean pooling，都没有需要学习的参数。因此，在卷积神经网络的训练中，Pooling层需要做的仅仅是将误差项传递到上一层，而没有梯度的计算
    - max pooling层：对于max pooling，下一层的误差项的值会原封不动的传递到上一层对应区块中的最大值所对应（前向传播的时候记录index索引，**关联知识:SegNet的反向上采样操作**）的神经元，而其他神经元的误差项的值都是0；
    - mean pooling层：对于mean pooling，下一层的误差项的值会平均分配到上一层对应区块中的所有神经元。
- with torch.no_grad()与不适用有什么区别：

  - with torch.no_grad()关闭之后不启用梯度更新与记录，节省算力
- 卷积如何梯度更新

  - 前向传播的时候，卷积操作就是线性操作，因此权重反向更新的时候就类似于全连接网络的梯度更新，使用loss的链式求导
- google使用的1\*3和3\*1代替3*3的卷积核原理：
  - 两者感受野一样，所以提取的特征是不受影响的
  - 相同的感受野，权重参数减少一般
- ResNet解决的什么问题
- 介绍以下ResNet中的block，解释他的作用：
  - ResNet有两种block，一种使用2个3\*3卷积，另一种是**1\*1+ 3*3 + 1\*1**交替结构**（重点考察）**
  - 主要会考察以下知识点：
    - 1\*1卷积的作用
    - **1\*1+ 3\*3 + 1\*1block**设计思想：
      - 第一个1\*1是为了通道降维，降维后使用3\*3计算量较小，然后使用1\*1还原通道，这种组合的感受野效果等效于2个3\*3卷积，同时参数两少了接近17倍
      - 交替使用是为了减少参数量能达到相同效果，1\*1改变维度与通道交换，3*3提取局部信息
- CNN的特点：
  - 局部链接+权重共享+汇聚层
- CNN中感受野/权重共享是什么意思？
  - 图像中的每一个像素点就是一个特征
  - 在卷积核不发生滑动计算的时候，feature map中的每一个权值对应一个感受野范围，该范围中的像素值由这个特征值负责，反过来说，就是这个感受野中的像素值共享了一个feature map上的value，也就是权重共享
  - 卷积核以滑动的方式计算特征 ，就是一个卷积核上的权重负责被整个图像的像素值共享去权重
- 如何解决模型不收敛问题
  - BN操作
  - 增加数据量
  - 减少参数量
  - 解决梯度爆炸，调整学习率

